{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.optim import Adam, RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.preprocessing import *\n",
    "from data.data_utils import *\n",
    "from models.macro_architectures import *\n",
    "from models.micro_architectures import *\n",
    "from utils import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = '20220801'\n",
    "end_time = '20230801'\n",
    "scrap_date = interval_time(start_time, end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months = list(set([day[:6] for day in scrap_date]))\n",
    "import_Dst(months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_sample, l2_sample, dst, kp = automated_preprocessing(scrap_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving missing values with interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time_ = '2022-08-01 00:00:00'\n",
    "end_time_ = '2023-08-03 23:59:00' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = '1T'\n",
    "\n",
    "full_time_index = pd.date_range(start=start_time_, end=end_time_, freq=freq)\n",
    "\n",
    "l1_sample = l1_sample.reindex(full_time_index)\n",
    "\n",
    "l1_sample = l1_sample.interpolate(method='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_sample = l2_sample.reindex(full_time_index)\n",
    "\n",
    "l2_sample = l2_sample.interpolate(method='linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L1 (raw) data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minute based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hour based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_sample_hour = l1_sample.resample('60min').mean()\n",
    "l1_sample_hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L2 (cleaned) data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minute based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hour based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_sample_hour = l2_sample.resample('60min').mean()\n",
    "l2_sample_hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dst data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kp data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets\n",
    "### Descriptions:\n",
    "**hn_dl**: hour normal dataloader\n",
    "\n",
    "**mn_dl**: minute normal dataloader\n",
    "\n",
    "**hr_dl**: minute normal dataloader\n",
    "\n",
    "**mr_dl**: minute normal dataloader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_default_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length_hour = 10  #hour\n",
    "sequence_length_minute = 600 #minute\n",
    "pred_length = 6 #hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Normal\n",
    "hour_Normal_dataset = NormalTrainingDataset(l1_sample_hour, dst, kp, sequence_length_hour, pred_length, hour = True)\n",
    "minute_Normal_dataset = NormalTrainingDataset(l1_sample, dst, kp, sequence_length_minute, pred_length, hour = False)\n",
    "##Refined(new method)\n",
    "hour_Refined_dataset = RefinedTrainingDataset(l1_sample_hour, l2_sample_hour, dst,kp,sequence_length_hour, pred_length, hour = True)\n",
    "minute_Refined_dataset = RefinedTrainingDataset(l1_sample, l2_sample, dst,kp,sequence_length_minute, pred_length, hour = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test:15% training: 85%\n",
    "\n",
    "test_size = round(0.15*len(hour_Normal_dataset))\n",
    "\n",
    "train_hn_ds, test_hn_ds = random_split(hour_Normal_dataset , [len(hour_Normal_dataset) - test_size, test_size])\n",
    "\n",
    "batch_size = 32  #Change based on GPU capacity\n",
    "\n",
    "train_hn_dl = DataLoader(train_hn_ds, batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "train_hn_dl = DeviceDataLoader(train_hn_dl, device)\n",
    "test_hn_dl = DataLoader(test_hn_ds, batch_size*2, num_workers=4, pin_memory=True)\n",
    "test_hn_dl = DeviceDataLoader(test_hn_dl, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test:15% training: 85%\n",
    "\n",
    "test_size = round(0.15*len(minute_Normal_dataset))\n",
    "\n",
    "train_mn_ds, test_mn_ds = random_split(minute_Normal_dataset , [len(minute_Normal_dataset) - test_size, test_size])\n",
    "\n",
    "batch_size = 32  #Change based on GPU capacity\n",
    "\n",
    "train_mn_dl = DataLoader(train_mn_ds, batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "train_mn_dl = DeviceDataLoader(train_mn_dl, device)\n",
    "test_mn_dl = DataLoader(test_mn_ds, batch_size*2, num_workers=4, pin_memory=True)\n",
    "test_mn_dl = DeviceDataLoader(test_mn_dl, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test:15% training: 85%\n",
    "\n",
    "test_size = round(0.15*len(hour_Refined_dataset))\n",
    "\n",
    "train_hr_ds, test_hr_ds = random_split(hour_Refined_dataset , [len(hour_Refined_dataset) - test_size, test_size])\n",
    "\n",
    "batch_size = 32  #Change based on GPU capacity\n",
    "\n",
    "train_hr_dl = DataLoader(train_hr_ds, batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "train_hr_dl = DeviceDataLoader(train_hr_dl, device)\n",
    "test_hr_dl = DataLoader(test_hr_ds, batch_size*2, num_workers=4, pin_memory=True)\n",
    "test_hr_dl = DeviceDataLoader(test_hr_dl, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test:15% training: 85%\n",
    "\n",
    "test_size = round(0.15*len(minute_Refined_dataset))\n",
    "\n",
    "train_mr_ds, test_mr_ds = random_split(minute_Refined_dataset , [len(minute_Refined_dataset) - test_size, test_size])\n",
    "\n",
    "batch_size = 32  #Change based on GPU capacity\n",
    "\n",
    "train_mr_dl = DataLoader(train_mr_ds, batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "train_mr_dl = DeviceDataLoader(train_mr_dl, device)\n",
    "test_mr_dl = DataLoader(test_mr_ds, batch_size*2, num_workers=4, pin_memory=True)\n",
    "test_mr_dl = DeviceDataLoader(test_mr_dl, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refined models with attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1d Convolutional encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Architecture\n",
    "hidden_size = 10\n",
    "input_size = 20\n",
    "##nn architecture\n",
    "architecture = (10,10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoders\n",
    "cnn_encoder_min = Simple1DCNN(architecture, input_size, hidden_size)\n",
    "cnn_encoder_hour = Simple1DCNN(architecture, input_size, hidden_size)\n",
    "#fc layer\n",
    "cnn_fc_dst_min = DeepNeuralNetwork(hidden_size, 1, *architecture)\n",
    "cnn_fc_kp_min = DeepNeuralNetwork(hidden_size, 29, *architecture)\n",
    "\n",
    "cnn_fc_dst_hour = DeepNeuralNetwork(hidden_size, 1, *architecture)\n",
    "cnn_fc_kp_hour = DeepNeuralNetwork(hidden_size, 29, *architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RefinedCNN_min = to_device(RefinedArchitecture(cnn_encoder_min, cnn_fc_dst_min, cnn_fc_kp_min), device)\n",
    "RefinedCNN_hour = to_device(RefinedArchitecture(cnn_encoder_hour, cnn_fc_dst_hour, cnn_fc_kp_hour), device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##hyperparameters\n",
    "epochs = 100\n",
    "max_lr = 1e-3\n",
    "weigth_decay = 1e-4\n",
    "grad_clip = 1e-2\n",
    "#opt_func = Adam\n",
    "opt_func = RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RefinedCNN_min_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RefinedCNN_min_history += RefinedCNN_min.fit(epochs, max_lr, train_mr_dl, test_mr_dl, weigth_decay, grad_clip, opt_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RefinedCNN_hour_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RefinedCNN_hour_history += RefinedCNN_hour.fit(epochs, max_lr, train_hr_dl, test_hr_dl, weigth_decay, grad_clip, opt_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional Deep LSTM encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Architecture\n",
    "hidden_size = 10\n",
    "input_size = 20\n",
    "##nn architecture\n",
    "architecture = (10,10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoders\n",
    "##Bidirectional minute based models\n",
    "deep_lstm_encoder_min_forward = DeepLSTM(hidden_size, input_size, batch_size, architecture)\n",
    "deep_lstm_encoder_min_backward = DeepLSTM(hidden_size, input_size, batch_size, architecture)\n",
    "bidirectional_deeplstm_encoder_min = BidirectionalRNNWithAttention(deep_lstm_encoder_min_forward, deep_lstm_encoder_min_backward)\n",
    "##Bidirectional hour based models\n",
    "deep_lstm_encoder_hour_forward = DeepLSTM(hidden_size, input_size, batch_size, architecture)\n",
    "deep_lstm_encoder_hour_backward = DeepLSTM(hidden_size, input_size, batch_size, architecture)\n",
    "bidirectional_deeplstm_encoder_hour = BidirectionalRNNWithAttention(deep_lstm_encoder_hour_forward, deep_lstm_encoder_hour_backward)\n",
    "#fc layer\n",
    "deep_lstm_fc_dst_min = DeepNeuralNetwork(2*hidden_size, 1, *architecture) #the multiplying factor because concatenating hidden_states on bidirectional arch\n",
    "deep_lstm_fc_kp_min = DeepNeuralNetwork(2*hidden_size, 29, *architecture)\n",
    "\n",
    "deep_lstm_fc_dst_hour = DeepNeuralNetwork(2*hidden_size, 1, *architecture)\n",
    "deep_lstm_fc_kp_hour = DeepNeuralNetwork(2*hidden_size, 29, *architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RefinedLSTM_min = to_device(RefinedArchitecture(bidirectional_deeplstm_encoder_min, deep_lstm_fc_dst_min, deep_lstm_fc_kp_min), device)\n",
    "RefinedLSTM_hour = to_device(RefinedArchitecture(bidirectional_deeplstm_encoder_hour, deep_lstm_fc_dst_hour, deep_lstm_fc_kp_hour), device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##hyperparameters\n",
    "epochs = 100\n",
    "max_lr = 1e-3\n",
    "weigth_decay = 1e-4\n",
    "grad_clip = 1e-2\n",
    "#opt_func = Adam\n",
    "opt_func = RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RefinedLSTM_min_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RefinedLSTM_min_history += RefinedLSTM_min.fit(epochs, max_lr, train_mr_dl, test_mr_dl, weigth_decay, grad_clip, opt_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RefinedLSTM_hour_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RefinedLSTM_hour_history += RefinedLSTM_hour.fit(epochs, max_lr, train_hr_dl, test_hr_dl, weigth_decay, grad_clip, opt_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional Deep GRU encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Architecture\n",
    "hidden_size = 10\n",
    "input_size = 20\n",
    "##nn architecture\n",
    "architecture = (10,10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoders\n",
    "##Bidirectional minute based models\n",
    "deep_gru_encoder_min_forward= DeepGRU(hidden_size, input_size, batch_size, architecture)\n",
    "deep_gru_encoder_min_backward= DeepGRU(hidden_size, input_size, batch_size, architecture)\n",
    "bidirectional_deepgru_encoder_min = BidirectionalRNNWithAttention(deep_gru_encoder_min_forward, deep_gru_encoder_min_backward)\n",
    "##Bidirectional hour based models\n",
    "deep_gru_encoder_hour_forward = DeepGRU(hidden_size, input_size, batch_size, architecture)\n",
    "deep_gru_encoder_hour_backward = DeepGRU(hidden_size, input_size, batch_size, architecture)\n",
    "bidirectional_deepgru_encoder_hour = BidirectionalRNNWithAttention(deep_gru_encoder_hour_forward, deep_gru_encoder_hour_backward)\n",
    "#fc layer\n",
    "deep_gru_fc_dst_min = DeepNeuralNetwork(2*hidden_size, 1, *architecture)\n",
    "deep_gru_fc_kp_min = DeepNeuralNetwork(2*hidden_size, 29, *architecture)\n",
    "\n",
    "deep_gru_fc_dst_hour = DeepNeuralNetwork(2*hidden_size, 1, *architecture)\n",
    "deep_gru_fc_kp_hour = DeepNeuralNetwork(2*hidden_size, 29, *architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RefinedGRU_min = to_device(RefinedArchitecture(bidirectional_deepgru_encoder_min, deep_gru_fc_dst_min, deep_gru_fc_kp_min), device)\n",
    "RefinedGRU_hour = to_device(RefinedArchitecture(bidirectional_deepgru_encoder_hour, deep_gru_fc_dst_hour, deep_gru_fc_kp_hour), device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##hyperparameters\n",
    "epochs = 100\n",
    "max_lr = 1e-3\n",
    "weigth_decay = 1e-4\n",
    "grad_clip = 1e-2\n",
    "#opt_func = Adam\n",
    "opt_func = RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RefinedGRU_min_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RefinedGRU_min_history += RefinedGRU_min.fit(epochs, max_lr, train_mr_dl, test_mr_dl, weigth_decay, grad_clip, opt_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RefinedGRU_hour_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RefinedGRU_hour_history += RefinedGRU_hour.fit(epochs, max_lr, train_hr_dl, test_hr_dl, weigth_decay, grad_clip, opt_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional Deep Vanilla RNN encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Architecture\n",
    "hidden_size = 10\n",
    "input_size = 20\n",
    "##nn architecture\n",
    "architecture = (10,10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoders\n",
    "##Bidirectional minute encoders\n",
    "deep_rnn_encoder_min_forward = DeepVanillaRNN(hidden_size, input_size, batch_size, architecture)\n",
    "deep_rnn_encoder_min_backward = DeepVanillaRNN(hidden_size, input_size, batch_size, architecture)\n",
    "bidirectional_deeprnn_encoder_min = BidirectionalRNNWithAttention(deep_rnn_encoder_min_forward,deep_rnn_encoder_min_backward)\n",
    "##Bidirectional hour encoders\n",
    "deep_rnn_encoder_hour_forward = DeepVanillaRNN(hidden_size, input_size, batch_size, architecture)\n",
    "deep_rnn_encoder_hour_backward = DeepVanillaRNN(hidden_size, input_size, batch_size, architecture)\n",
    "bidirectional_deeprnn_encoder_hour = BidirectionalRNNWithAttention(deep_rnn_encoder_hour_forward,deep_rnn_encoder_hour_backward)\n",
    "#fc layer\n",
    "deep_rnn_fc_dst_min = DeepNeuralNetwork(2*hidden_size, 1, *architecture)\n",
    "deep_rnn_fc_kp_min = DeepNeuralNetwork(2*hidden_size, 29, *architecture)\n",
    "\n",
    "deep_rnn_fc_dst_hour = DeepNeuralNetwork(2*hidden_size, 1, *architecture)\n",
    "deep_rnn_fc_kp_hour = DeepNeuralNetwork(2*hidden_size, 29, *architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RefinedVanillaRNN_min = to_device(RefinedArchitecture(bidirectional_deeprnn_encoder_min, deep_rnn_fc_dst_min, deep_rnn_fc_kp_min), device)\n",
    "RefinedVanillaRNN_hour = to_device(RefinedArchitecture(bidirectional_deeprnn_encoder_hour, deep_rnn_fc_dst_hour, deep_rnn_fc_kp_hour), device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##hyperparameters\n",
    "epochs = 100\n",
    "max_lr = 1e-3\n",
    "weigth_decay = 1e-4\n",
    "grad_clip = 1e-2\n",
    "#opt_func = Adam\n",
    "opt_func = RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RefinedVanillaRNN_min_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RefinedVanillaRNN_min_history += RefinedVanillaRNN_min.fit(epochs, max_lr, train_mr_dl, test_mr_dl, weigth_decay, grad_clip, opt_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RefinedVanillaRNN_hour_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RefinedVanillaRNN_hour_history += RefinedVanillaRNN_hour.fit(epochs, max_lr, train_hr_dl, test_hr_dl, weigth_decay, grad_clip, opt_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional non deep architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_deep_lstm_forward_min = LSTM(input_size, hidden_size)\n",
    "non_deep_lstm_backward_min = LSTM(input_size, hidden_size)\n",
    "bidirectional_lstm_encoder_min = BidirectionalRNNWithAttention(non_deep_lstm_forward_min, non_deep_lstm_backward_min)\n",
    "\n",
    "non_deep_lstm_forward_hour = LSTM(input_size, hidden_size)\n",
    "non_deep_lstm_backward_hour = LSTM(input_size, hidden_size)\n",
    "bidirectional_lstm_encoder_hour = BidirectionalRNNWithAttention(non_deep_lstm_forward_hour, non_deep_lstm_backward_hour)\n",
    "#fc layer\n",
    "non_deep_lstm_fc_dst_min = DeepNeuralNetwork(2*hidden_size, 1, *architecture)\n",
    "non_deep_lstm_fc_kp_min = DeepNeuralNetwork(2*hidden_size, 29, *architecture)\n",
    "\n",
    "non_deep_lstm_fc_dst_hour = DeepNeuralNetwork(2*hidden_size, 1, *architecture)\n",
    "non_deep_lstm_fc_kp_hour = DeepNeuralNetwork(2*hidden_size, 29, *architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RefinedNonDeepLSTM_min = to_device(RefinedArchitecture(bidirectional_lstm_encoder_min, non_deep_lstm_fc_dst_min, non_deep_lstm_fc_kp_min), device)\n",
    "RefinedNonDeepLSTM_hour = to_device(RefinedArchitecture(bidirectional_lstm_encoder_hour, non_deep_lstm_fc_dst_hour, non_deep_lstm_fc_kp_hour), device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RefinedNonDeepLSTM_min_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RefinedNonDeepLSTM_min_history += RefinedNonDeepLSTM_min.fit(epochs, max_lr, train_mr_dl, test_mr_dl, weigth_decay, grad_clip, opt_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RefinedNonDeepLSTM_hour_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RefinedNonDeepLSTM_hour_history += RefinedNonDeepLSTM_hour.fit(epochs, max_lr, train_hr_dl, test_hr_dl, weigth_decay, grad_clip, opt_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_deep_gru_forward_min = GRU(input_size, hidden_size)\n",
    "non_deep_gru_backward_min = GRU(input_size, hidden_size)\n",
    "bidirectional_gru_encoder_min = BidirectionalRNNWithAttention(non_deep_gru_forward_min, non_deep_gru_backward_min)\n",
    "\n",
    "non_deep_gru_forward_hour = GRU(input_size, hidden_size)\n",
    "non_deep_gru_backward_hour = GRU(input_size, hidden_size)\n",
    "bidirectional_gru_encoder_hour = BidirectionalRNNWithAttention(non_deep_gru_forward_hour, non_deep_gru_backward_hour)\n",
    "#fc layer\n",
    "non_deep_gru_fc_dst_min = DeepNeuralNetwork(2*hidden_size, 1, *architecture)\n",
    "non_deep_gru_fc_kp_min = DeepNeuralNetwork(2*hidden_size, 29, *architecture)\n",
    "\n",
    "non_deep_gru_fc_dst_hour = DeepNeuralNetwork(2*hidden_size, 1, *architecture)\n",
    "non_deep_gru_fc_kp_hour = DeepNeuralNetwork(2*hidden_size, 29, *architecture)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RefinedNonDeepGRU_min = to_device(RefinedArchitecture(bidirectional_gru_encoder_min, non_deep_gru_fc_dst_min, non_deep_gru_fc_kp_min), device)\n",
    "RefinedNonDeepGRU_hour = to_device(RefinedArchitecture(bidirectional_gru_encoder_hour, non_deep_gru_fc_dst_hour, non_deep_gru_fc_kp_hour), device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RefinedNonDeepGRU_min_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RefinedNonDeepGRU_min_history += RefinedNonDeepGRU_min.fit(epochs, max_lr, train_mr_dl, test_mr_dl, weigth_decay, grad_clip, opt_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RefinedNonDeepGRU_hour_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RefinedNonDeepGRU_hour_history += RefinedNonDeepGRU_hour.fit(epochs, max_lr, train_hr_dl, test_hr_dl, weigth_decay, grad_clip, opt_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_deep_rnn_forward_min = VanillaRNN(input_size, hidden_size)\n",
    "non_deep_rnn_backward_min = VanillaRNN(input_size, hidden_size)\n",
    "bidirectional_rnn_encoder_min = BidirectionalRNNWithAttention(non_deep_rnn_forward_min, non_deep_rnn_backward_min)\n",
    "\n",
    "non_deep_rnn_forward_hour = VanillaRNN(input_size, hidden_size)\n",
    "non_deep_rnn_backward_hour = VanillaRNN(input_size, hidden_size)\n",
    "bidirectional_rnn_encoder_hour = BidirectionalRNNWithAttention(non_deep_rnn_forward_hour, non_deep_rnn_backward_hour)\n",
    "#fc layer\n",
    "non_deep_rnn_fc_dst_min = DeepNeuralNetwork(2*hidden_size, 1, *architecture)\n",
    "non_deep_rnn_fc_kp_min = DeepNeuralNetwork(2*hidden_size, 29, *architecture)\n",
    "\n",
    "non_deep_rnn_fc_dst_hour = DeepNeuralNetwork(2*hidden_size, 1, *architecture)\n",
    "non_deep_rnn_fc_kp_hour = DeepNeuralNetwork(2*hidden_size, 29, *architecture)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RefinedNonDeepVanillaRNN_min = to_device(RefinedArchitecture(bidirectional_rnn_encoder_min, non_deep_rnn_fc_dst_min, non_deep_rnn_fc_kp_min), device)\n",
    "RefinedNonDeepVanillaRNN_hour = to_device(RefinedArchitecture(bidirectional_rnn_encoder_hour, non_deep_rnn_fc_dst_hour, non_deep_rnn_fc_kp_hour), device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RefinedNonDeepVanillaRNN_min_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RefinedNonDeepVanillaRNN_min_history += RefinedNonDeepVanillaRNN_min.fit(epochs, max_lr, train_mr_dl, test_mr_dl, weigth_decay, grad_clip, opt_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RefinedNonDeepVanillaRNN_hour_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RefinedNonDeepVanillaRNN_hour_history += RefinedNonDeepVanillaRNN_hour.fit(epochs, max_lr, train_hr_dl, test_hr_dl, weigth_decay, grad_clip, opt_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal models with attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1d Convolutional encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Architecture\n",
    "hidden_size = 10\n",
    "input_size = 20\n",
    "##nn architecture\n",
    "architecture = (16,8,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoders\n",
    "cnn_encoder_min = Simple1DCNN(architecture, input_size, hidden_size)\n",
    "cnn_encoder_hour = Simple1DCNN(architecture, input_size, hidden_size)\n",
    "#fc layer\n",
    "cnn_fc_dst_min = DeepNeuralNetwork(hidden_size, 1, *architecture)\n",
    "cnn_fc_kp_min = DeepNeuralNetwork(hidden_size, 29, *architecture)\n",
    "\n",
    "cnn_fc_dst_hour = DeepNeuralNetwork(hidden_size, 1, *architecture)\n",
    "cnn_fc_kp_hour = DeepNeuralNetwork(hidden_size, 29, *architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NormalCNN_min = to_device(NormalArchitecture(cnn_encoder_min, cnn_fc_dst_min, cnn_fc_kp_min), device)\n",
    "NormalCNN_hour = to_device(NormalArchitecture(cnn_encoder_hour, cnn_fc_dst_hour, cnn_fc_kp_hour), device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##hyperparameters\n",
    "epochs = 100\n",
    "max_lr = 1e-3\n",
    "weigth_decay = 1e-4\n",
    "grad_clip = 1e-2\n",
    "#opt_func = Adam\n",
    "opt_func = RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NormalCNN_min_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NormalCNN_min_history += NormalCNN_min.fit(epochs, max_lr, train_mn_dl, test_mn_dl, weigth_decay, grad_clip, opt_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NormalCNN_hour_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NormalCNN_hour_history += NormalCNN_hour.fit(epochs, max_lr, train_hn_dl, test_hn_dl, weigth_decay, grad_clip, opt_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional Deep LSTM encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Architecture\n",
    "hidden_size = 10\n",
    "input_size = 20\n",
    "##nn architecture\n",
    "architecture = (10,10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoders\n",
    "deep_lstm_encoder_min = DeepLSTM(hidden_size, input_size, batch_size, architecture)\n",
    "deep_lstm_encoder_hour = DeepLSTM(hidden_size, input_size, batch_size, architecture)\n",
    "#fc layer\n",
    "deep_lstm_fc_dst_min = DeepNeuralNetwork(hidden_size, 1, *architecture)\n",
    "deep_lstm_fc_kp_min = DeepNeuralNetwork(hidden_size, 29, *architecture)\n",
    "\n",
    "deep_lstm_fc_dst_hour = DeepNeuralNetwork(hidden_size, 1, *architecture)\n",
    "deep_lstm_fc_kp_hour = DeepNeuralNetwork(hidden_size, 29, *architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NormalLSTM_min = to_device(NormalArchitecture(deep_lstm_encoder_min, deep_lstm_fc_dst_min, deep_lstm_fc_kp_min), device)\n",
    "NormalLSTM_hour = to_device(NormalArchitecture(deep_lstm_encoder_hour, deep_lstm_fc_dst_hour, deep_lstm_fc_kp_hour), device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##hyperparameters\n",
    "epochs = 100\n",
    "max_lr = 1e-3\n",
    "weigth_decay = 1e-4\n",
    "grad_clip = 1e-2\n",
    "#opt_func = Adam\n",
    "opt_func = RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NormalLSTM_min_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NormalLSTM_min_history += NormalLSTM_min.fit(epochs, max_lr, train_mn_dl, test_mn_dl, weigth_decay, grad_clip, opt_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NormalLSTM_hour_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NormalLSTM_hour_history += NormalLSTM_hour.fit(epochs, max_lr, train_hn_dl, test_hn_dl, weigth_decay, grad_clip, opt_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional Deep GRU encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Architecture\n",
    "hidden_size = 10\n",
    "input_size = 20\n",
    "##nn architecture\n",
    "architecture = (10,10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoders\n",
    "deep_gru_encoder_min = DeepGRU(hidden_size, input_size, batch_size, architecture)\n",
    "deep_gru_encoder_hour = DeepGRU(hidden_size, input_size, batch_size, architecture)\n",
    "#fc layer\n",
    "deep_gru_fc_dst_min = DeepNeuralNetwork(hidden_size, 1, *architecture)\n",
    "deep_gru_fc_kp_min = DeepNeuralNetwork(hidden_size, 29, *architecture)\n",
    "\n",
    "deep_gru_fc_dst_hour = DeepNeuralNetwork(hidden_size, 1, *architecture)\n",
    "deep_gru_fc_kp_hour = DeepNeuralNetwork(hidden_size, 29, *architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NormalGRU_min = to_device(NormalArchitecture(deep_gru_encoder_min, deep_gru_fc_dst_min, deep_gru_fc_kp_min), device)\n",
    "NormalGRU_hour = to_device(NormalArchitecture(deep_gru_encoder_hour, deep_gru_fc_dst_hour, deep_gru_fc_kp_hour), device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##hyperparameters\n",
    "epochs = 100\n",
    "max_lr = 1e-3\n",
    "weigth_decay = 1e-4\n",
    "grad_clip = 1e-2\n",
    "#opt_func = Adam\n",
    "opt_func = RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NormalGRU_min_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NormalGRU_min_history += NormalGRU_min.fit(epochs, max_lr, train_mn_dl, test_mn_dl, weigth_decay, grad_clip, opt_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NormalGRU_hour_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NormalGRU_hour_history += NormalGRU_hour.fit(epochs, max_lr, train_hn_dl, test_hn_dl, weigth_decay, grad_clip, opt_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional Deep Vanilla RNN encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Architecture\n",
    "hidden_size = 10\n",
    "input_size = 20\n",
    "##nn architecture\n",
    "architecture = (10,10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoders\n",
    "deep_rnn_encoder_min = DeepVanillaRNN(hidden_size, input_size, batch_size, architecture)\n",
    "deep_rnn_encoder_hour = DeepVanillaRNN(hidden_size, input_size, batch_size, architecture)\n",
    "#fc layer\n",
    "deep_rnn_fc_dst_min = DeepNeuralNetwork(hidden_size, 1, *architecture)\n",
    "deep_rnn_fc_kp_min = DeepNeuralNetwork(hidden_size, 29, *architecture)\n",
    "\n",
    "deep_rnn_fc_dst_hour = DeepNeuralNetwork(hidden_size, 1, *architecture)\n",
    "deep_rnn_fc_kp_hour = DeepNeuralNetwork(hidden_size, 29, *architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NormalVanillaRNN_min = to_device(RefinedArchitecture(deep_rnn_encoder_min, deep_rnn_fc_dst_min, deep_rnn_fc_kp_min), device)\n",
    "NormalVanillaRNN_hour = to_device(RefinedArchitecture(deep_rnn_encoder_hour, deep_rnn_fc_dst_hour, deep_rnn_fc_kp_hour), device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##hyperparameters\n",
    "epochs = 100\n",
    "max_lr = 1e-3\n",
    "weigth_decay = 1e-4\n",
    "grad_clip = 1e-2\n",
    "#opt_func = Adam\n",
    "opt_func = RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NormalVanillaRNN_min_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NormalVanillaRNN_min_history += NormalVanillaRNN_min.fit(epochs, max_lr, train_mn_dl, test_mn_dl, weigth_decay, grad_clip, opt_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NormalVanillaRNN_hour_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NormalVanillaRNN_hour_history += NormalVanillaRNN_hour.fit(epochs, max_lr, train_hn_dl, test_hn_dl, weigth_decay, grad_clip, opt_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non deep bidirectional architectures with attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_deep_lstm_forward_min = LSTM(input_size, hidden_size)\n",
    "non_deep_lstm_backward_min = LSTM(input_size, hidden_size)\n",
    "bidirectional_lstm_encoder_min = BidirectionalRNNWithAttention(non_deep_lstm_forward_min, non_deep_lstm_backward_min)\n",
    "\n",
    "non_deep_lstm_forward_hour = LSTM(input_size, hidden_size)\n",
    "non_deep_lstm_backward_hour = LSTM(input_size, hidden_size)\n",
    "bidirectional_lstm_encoder_hour = BidirectionalRNNWithAttention(non_deep_lstm_forward_hour, non_deep_lstm_backward_hour)\n",
    "#fc layer\n",
    "non_deep_lstm_fc_dst_min = DeepNeuralNetwork(2*hidden_size, 1, *architecture)\n",
    "non_deep_lstm_fc_kp_min = DeepNeuralNetwork(2*hidden_size, 29, *architecture)\n",
    "\n",
    "non_deep_lstm_fc_dst_hour = DeepNeuralNetwork(2*hidden_size, 1, *architecture)\n",
    "non_deep_lstm_fc_kp_hour = DeepNeuralNetwork(2*hidden_size, 29, *architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NormalNonDeepLSTM_min = to_device(NormalArchitecture(bidirectional_lstm_encoder_min, non_deep_lstm_fc_dst_min, non_deep_lstm_fc_kp_min), device)\n",
    "NormalNonDeepLSTM_hour = to_device(NormalArchitecture(bidirectional_lstm_encoder_hour, non_deep_lstm_fc_dst_hour, non_deep_lstm_fc_kp_hour), device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NormalNonDeepLSTM_min_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NormalNonDeepLSTM_min_history += NormalNonDeepLSTM_min.fit(epochs, max_lr, train_mr_dl, test_mr_dl, weigth_decay, grad_clip, opt_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NormalNonDeepLSTM_hour_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NormalNonDeepLSTM_hour_history += NormalNonDeepLSTM_hour.fit(epochs, max_lr, train_hr_dl, test_hr_dl, weigth_decay, grad_clip, opt_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_deep_gru_forward_min = GRU(input_size, hidden_size)\n",
    "non_deep_gru_backward_min = GRU(input_size, hidden_size)\n",
    "bidirectional_gru_encoder_min = BidirectionalRNNWithAttention(non_deep_gru_forward_min, non_deep_gru_backward_min)\n",
    "\n",
    "non_deep_gru_forward_hour = GRU(input_size, hidden_size)\n",
    "non_deep_gru_backward_hour = GRU(input_size, hidden_size)\n",
    "bidirectional_gru_encoder_hour = BidirectionalRNNWithAttention(non_deep_gru_forward_hour, non_deep_gru_backward_hour)\n",
    "#fc layer\n",
    "non_deep_gru_fc_dst_min = DeepNeuralNetwork(2*hidden_size, 1, *architecture)\n",
    "non_deep_gru_fc_kp_min = DeepNeuralNetwork(2*hidden_size, 29, *architecture)\n",
    "\n",
    "non_deep_gru_fc_dst_hour = DeepNeuralNetwork(2*hidden_size, 1, *architecture)\n",
    "non_deep_gru_fc_kp_hour = DeepNeuralNetwork(2*hidden_size, 29, *architecture)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NormalNonDeepGRU_min = to_device(NormalArchitecture(bidirectional_gru_encoder_min, non_deep_gru_fc_dst_min, non_deep_gru_fc_kp_min), device)\n",
    "NormalNonDeepGRU_hour = to_device(NormalArchitecture(bidirectional_gru_encoder_hour, non_deep_gru_fc_dst_hour, non_deep_gru_fc_kp_hour), device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NormalNonDeepGRU_min_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NormalNonDeepGRU_min_history += NormalNonDeepGRU_min.fit(epochs, max_lr, train_mr_dl, test_mr_dl, weigth_decay, grad_clip, opt_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NormalNonDeepGRU_hour_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NormalNonDeepGRU_hour_history += NormalNonDeepGRU_hour.fit(epochs, max_lr, train_hr_dl, test_hr_dl, weigth_decay, grad_clip, opt_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_deep_rnn_forward_min = VanillaRNN(input_size, hidden_size)\n",
    "non_deep_rnn_backward_min = VanillaRNN(input_size, hidden_size)\n",
    "bidirectional_rnn_encoder_min = BidirectionalRNNWithAttention(non_deep_rnn_forward_min, non_deep_rnn_backward_min)\n",
    "\n",
    "non_deep_rnn_forward_hour = VanillaRNN(input_size, hidden_size)\n",
    "non_deep_rnn_backward_hour = VanillaRNN(input_size, hidden_size)\n",
    "bidirectional_rnn_encoder_hour = BidirectionalRNNWithAttention(non_deep_rnn_forward_hour, non_deep_rnn_backward_hour)\n",
    "#fc layer\n",
    "non_deep_rnn_fc_dst_min = DeepNeuralNetwork(2*hidden_size, 1, *architecture)\n",
    "non_deep_rnn_fc_kp_min = DeepNeuralNetwork(2*hidden_size, 29, *architecture)\n",
    "\n",
    "non_deep_rnn_fc_dst_hour = DeepNeuralNetwork(2*hidden_size, 1, *architecture)\n",
    "non_deep_rnn_fc_kp_hour = DeepNeuralNetwork(2*hidden_size, 29, *architecture)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NormalNonDeepVanillaRNN_min = to_device(NormalArchitecture(bidirectional_rnn_encoder_min, non_deep_rnn_fc_dst_min, non_deep_rnn_fc_kp_min), device)\n",
    "NormalNonDeepVanillaRNN_hour = to_device(NormalArchitecture(bidirectional_rnn_encoder_hour, non_deep_rnn_fc_dst_hour, non_deep_rnn_fc_kp_hour), device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NormalNonDeepVanillaRNN_min_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RefinedNonDeepVanillaRNN_min_history += NormalNonDeepVanillaRNN_min.fit(epochs, max_lr, train_mr_dl, test_mr_dl, weigth_decay, grad_clip, opt_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NormalNonDeepVanillaRNN_hour_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NormalNonDeepVanillaRNN_hour_history += NormalNonDeepVanillaRNN_hour.fit(epochs, max_lr, train_hr_dl, test_hr_dl, weigth_decay, grad_clip, opt_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
